{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"The information was used as the basis for decision\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of tokens list is: 9 \n",
      " Tokens are: ['the', 'information', 'was', 'used', 'as', 'the', 'basis', 'for', 'decision']\n"
     ]
    }
   ],
   "source": [
    "tokens = list(sentence.lower().split())\n",
    "print(f\"Length of tokens list is: {len(tokens)} \\n Tokens are: {tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<pad>': 0, 'the': 1, 'information': 2, 'was': 3, 'used': 4, 'as': 5, 'basis': 6, 'for': 7, 'decision': 8}\n"
     ]
    }
   ],
   "source": [
    "vocab, index = {}, 1 # start index from 1\n",
    "vocab['<pad>'] = 0 # index for padding token is 0\n",
    "for token in tokens:\n",
    "    if token not in vocab:\n",
    "        vocab[token] = index\n",
    "        index += 1\n",
    "vocab_size = len(vocab)\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: '<pad>', 1: 'the', 2: 'information', 3: 'was', 4: 'used', 5: 'as', 6: 'basis', 7: 'for', 8: 'decision'}\n"
     ]
    }
   ],
   "source": [
    "# Inverse vocab to save mappings from integer indices to tokens\n",
    "\n",
    "inverse_vocab = {index: token for token, index in vocab.items()}\n",
    "print(inverse_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5, 1, 6, 7, 8]\n"
     ]
    }
   ],
   "source": [
    "test_sentence = [vocab[word] for word in tokens]\n",
    "print(test_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n"
     ]
    }
   ],
   "source": [
    "# Generate skip-grams from one sentence\n",
    "\n",
    "window_size = 2\n",
    "positive_skip_grams, _ = tf.keras.preprocessing.sequence.skipgrams(\n",
    "    test_sentence,\n",
    "    vocabulary_size= vocab_size,\n",
    "    window_size = window_size,\n",
    "    negative_samples = 0\n",
    "    )\n",
    "print(len(positive_skip_grams))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6, 5): (basis, as)\n",
      "(3, 4): (was, used)\n",
      "(3, 5): (was, as)\n",
      "(1, 7): (the, for)\n",
      "(7, 8): (for, decision)\n",
      "(4, 1): (used, the)\n",
      "(6, 8): (basis, decision)\n",
      "(8, 7): (decision, for)\n",
      "(1, 5): (the, as)\n",
      "(6, 1): (basis, the)\n",
      "(5, 6): (as, basis)\n",
      "(2, 4): (information, used)\n",
      "(1, 2): (the, information)\n",
      "(5, 3): (as, was)\n",
      "(3, 1): (was, the)\n",
      "(3, 2): (was, information)\n",
      "(1, 3): (the, was)\n",
      "(7, 6): (for, basis)\n",
      "(4, 5): (used, as)\n",
      "(6, 7): (basis, for)\n",
      "(4, 3): (used, was)\n",
      "(2, 3): (information, was)\n",
      "(5, 4): (as, used)\n",
      "(7, 1): (for, the)\n",
      "(1, 6): (the, basis)\n",
      "(4, 2): (used, information)\n",
      "(2, 1): (information, the)\n",
      "(5, 1): (as, the)\n",
      "(1, 4): (the, used)\n",
      "(8, 6): (decision, basis)\n"
     ]
    }
   ],
   "source": [
    "for target, context in positive_skip_grams[:]:\n",
    "    print(f\"({target}, {context}): ({inverse_vocab[target]}, {inverse_vocab[context]})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
